---
title: "Report"
output: html_notebook
---

# Questions for Daniel

- Which way of calculating $I^2$ to use?

- Can I access the 'back-end' of TOSTER?

- 


# What is $I^2$?

*The first step in partitioning the variation is to compute Q, defined as* 
$$
Q=\sum_{i=1}^{k} W_{i}\left(Y_{i}-M\right)^{2}
$$
*where Wi is the study weight (1/Vi), Yi is the study effect size, and M is the summary effect and k is the number of studies. In words, we compute the deviation of each effect size from the mean, square it, weight this by the inverse-variance for that study, and sum these values over all studies to yield the weighted sum of squares (WSS), or Q. The same formula can be written as*
$$
Q=\sum_{i=1}^{k}\left(\frac{Y_{i}-M}{S_{i}}\right)^{2}
$$

*to highlight the fact that Q is a standardized measure, which means that it is not affected by the metric of the effect size index. The analogy would be to the standardized mean difference d, where the mean difference is divided by the within-study standard deviation. Finally, an equivalent formula, useful for computations, is*
$$
Q=\sum_{i=1}^{k} W_{i} Y_{i}^{2}-\frac{\left(\sum_{i=1}^{k} W_{i} Y_{i}\right)^{2}}{\sum_{i=1}^{k} W_{i}}.
$$
(Borenstein, p. 109)


*The next step is to determine the expected value of Q on the assumption that all studies share a common effect size, and (it follows) all the variation is due to sampling error within studies. Because Q is a standardized measure the expected value does not depend on the metric of effect size, but is simply the degrees of freedom (df), *

$$
df = k-1,
$$
*where k is the number of studies.* (Borenstein, p. 110)


*[T]o estimate what proportion of the observed variance reflects real differences among studies (rather than random error) we will start with Q, remove the dependence on the number of studies, and express the result as a ratio (called $I^2$).* (Borenstein, p. 112)

*What proportion of the observed variance reflects real differences in effect size? Higgins et al. (2003) proposed using a statistic, $I^2$ , to reflect this proportion, that could serve as a kind of signal-to-noise ratio. It is computed as*
$$
I^{2}=\left(\frac{Q-d f}{Q}\right) \times 100 \%
$$
*that is, the ratio of excess dispersion to total dispersion.* (Borenstein, p. 117)



*$I^2$ reflects the extent of overlap of confidence intervals, which is not dependent on the actual location or spread of the true effects. As such it is convenient to view $I^2$ as a measure of inconsistency across the findings of the studies, and not as a measure of the real variation across the underlying true effects. The scale of $I^2$ has a range of 0–100%, regardless of the scale used for the metaanalysis itself. It can be interpreted as a ratio, and has the additional advantage of being analogous to indices used in psychometrics (where reliability is the ratio of true to total variance) or regression (where R2 is the proportion of the total variance that can be explained by the covariates). Importantly, $I^2$ is not directly affected by the number of studies in the analysis.* (Borenstein, p. 118)

...

*Higgins et al. (2003) provide some tentative benchmarks for $I^2$ . They suggest that values on the order of 25%, 50%, and 75% might be considered as low, moderate, and high, respectively. Some context for the interpretation of $I^2$ is provided by a survey of meta-analyses of clinical trials in the Cochrane Database of Systematic Reviews, reported by Higgins et al. (2003). The value of $I^2$ was zero for about half of the meta-analyses, and was distributed evenly between 0% to 100% for the other half. It is likely that $I^2$ would be distributed differently in meta-analyses of other fields or other kinds of studies.* (Borenstein, p. 119)


# Comupting $I^2$

Create some data to test equations:
```{r}
# Simulate data:
ES <- rnorm(n = 5, mean = .5, sd = .5) #effect size per study
SE <- runif(n = 5, min = 0, max = 0.05) #standard error per study
n <- 100
data_sim <- as.data.frame(cbind(ES, SE)) #combine into dataframe

# Obtain objects necessary for computing I^2:
Y <- data_sim$ES
k <- length(Y) #extract nr. of studies
df <- k-1 #calculate degrees of freedom 
V <- (data_sim$SE*sqrt(n))^2
W <- 1/V #create study weights
M <- mean(Y)
W_alt <- rnorm(n = 5, mean = 100, sd = 50)

# Compute Q:
Qa <- sum(W*(Y-M)^2) #first equation in Borenstein
Qi <- matrix(NA, k, 1) #same with for-loop to check computations
for (i in 1:k) {
  Qi[i] <- W[i]*((Y[i]-M)^2)
}
Qb <- sum(Qi)

Qc <- sum(W*Y^2)-((sum(W*Y)^2)/sum(W)) #third Borenstein equation

# Compute I^2:
Q <- Qc #choose which Q computation to use
I2 <- (Q-df)/Q *100 #get I^2

# Use function (see below):
# sim_out <- Q_stat(yi = Y, wi = W)

# Check with package:
sim <- metafor::rma.uni(yi = Y, vi = 1/W)

```

Try with real data:
```{r}
data <- read.csv2("Conditioning.csv") #load data into environment
Y <- data$Calculated.effect.size #extract effect size, SE and n per study
SE <- data$SE.rounded 
n <- data$Sample.size

k <- length(Y) #extract nr. of studies
df <- k-1 #calculate degrees of freedom 

SD <- SE*sqrt(n) #calculate SD from SE
W <- 1/(SD^2) #create study weights (inverse of variance)
M <- mean(Y)

# Compute Q:
Qa <- sum(W*(Y-M)^2) #first equation in Borenstein
Qi <- matrix(NA, k, 1) #same with for-loop to check computations
for (i in 1:k) {
  Qi[i] <- W[i]*((Y[i]-M)^2)
}
Qb <- sum(Qi)

Qc <- sum(W*Y^2)-((sum(W*Y)^2)/sum(W)) #third Borenstein equation

# Compute I^2:
Q <- Qc #choose which Q computation to use
I2 <- (Q-df)/Q*100 #get I^2

# Check with package:
out <- metafor::rma.uni(yi = Y, vi = 1/W)

# We see that Qc yields a Q value stat is equal to the metafor Q estimate
# However, the I^2 estimate is not equal.

```

Source ToM data: https://psyarxiv.com/97yjx/, source conditioning data: https://psyarxiv.com/qz5st/ and https://osf.io/dy4ac/files/?view_only=aecccafd3dd44fefa17cf87aeb8f92ac.

```{r}
# Create function for Q:

Q_stat <- function(yi, wi){ #possibly replace with SE and var to match TOSTER variables
  # This function yields the Q and I^2 statistic to evaluate heterogeneity in meta-analyses.
  # As input it needs a vector with estimated effect sizes per study, and study weights 
  # (inverse of the variance).
  # The output of the function is a list of two objects: the estimated Q statistic, and 
  # the I^2 estimate.
  
  # Specify predefined elements:
  k <- length(Y) #extract nr. of studies
  df <- k-1 #calculate degrees of freedom 

  # Calculate Q:
  Q <- sum(wi*yi^2)-((sum(wi*yi)^2)/sum(wi)) #equation Borenstein p. 109
  
  # Calculate I^2:
  I2 <- 100 * ((Q-df)/Q)
  
  # Create objects for output:
  
  
  # Put everything into list to print:
  output <- list(
    "Q" = Q,
    "I2" = I2
  )
  
  return(output)
}

# Test function

Q_out <- Q_stat(yi = Y, wi = W)

```


Now use the TOSTER format, see https://www.rdocumentation.org/packages/TOSTER/versions/0.3.4:

```{r}
library('TOSTER')

TOSTI<-function(ES,var,se,low_eqbound_d, high_eqbound_d, alpha #, plot = TRUE, verbose = TRUE
                ){
  
  if(length(ES) < 2) stop("The sample size should be larger than 1.")
  if(missing(alpha)) {
    alpha<-0.05
  }
  if(missing(se)) {
    if(missing(var)) {
      stop("Need to specify variance (var) or standard error (se) per study.")
    }
    se<-sqrt(var)
  }
  if(missing(var)) {
    if(missing(se)) {
      stop("Need to specify variance (var) or standard error (se) per study.")
    }
  }
  if(low_eqbound_d >= high_eqbound_d) warning("The lower bound is equal to or larger than the upper bound. Check the plot and output to see if the bounds are specified as you intended.")
  if(1 <= alpha | alpha <= 0) stop("The alpha level should be a positive value between 0 and 1.")
  
  # Specify elements in equation:
  df <- length(ES) - 1 #degrees of freedom = nr. of studies minus one
  W <- 1/(se^2) #study weights are the inverse of the variance
  Q <- sum(W*ES^2)-((sum(W*ES)^2)/sum(W)) #equation Borenstein p. 109
  p_Q <- pchisq(Q, df=df, lower.tail=FALSE)
  I2 <- 100 * ((Q-df)/Q) #equation Borenstein p. 117

  #######################################  
  # TOSTER code from TOSTmeta.R function:
  
#   Z1<-(ES-low_eqbound_d)/se
#   p1<-pnorm(Z1, lower.tail=FALSE)
#   Z2<-(ES-high_eqbound_d)/se
#   p2<-pnorm(Z2, lower.tail=TRUE)
#   Z<-(ES/se)
#   pttest<-2*pnorm(-abs(Z))
#   LL90<-ES-qnorm(1-alpha)*(se)
#   UL90<-ES+qnorm(1-alpha)*(se)
#   LL95<-ES-qnorm(1-alpha/2)*(se)
#   UL95<-ES+qnorm(1-alpha/2)*(se)
#   ptost<-max(p1,p2) #Get highest p-value for summary TOST result
#   Ztost<-ifelse(abs(Z1) < abs(Z2), Z1, Z2) #Get lowest t-value for summary TOST result
  
#   results<-data.frame(Z1,p1,Z2,p2,LL90,UL90)
#   colnames(results) <- c("Z-value 1","p-value 1","Z-value 2","p-value 2", paste("Lower Limit ",100*(1-alpha*2),"% CI",sep=""),paste("Upper Limit ",100*(1-alpha*2),"% CI",sep=""))
#   testoutcome<-ifelse(pttest<alpha,"significant","non-significant")
#   TOSToutcome<-ifelse(ptost<alpha,"significant","non-significant")
  
#   # Plot results
#   if (plot == TRUE) {
#   plot(NA, ylim=c(0,1), xlim=c(min(LL95,low_eqbound_d,ES)-max(UL95-LL95, high_eqbound_d-low_eqbound_d,ES)/10, max(UL95,high_eqbound_d,ES)+max(UL95-LL95, high_eqbound_d-low_eqbound_d, ES)/10), bty="l", yaxt="n", ylab="",xlab="Effect size")
#   points(x=ES, y=0.5, pch=15, cex=2)
#   abline(v=high_eqbound_d, lty=2)
#   abline(v=low_eqbound_d, lty=2)
#   abline(v=0, lty=2, col="grey")
#   segments(LL90,0.5,UL90,0.5, lwd=3)
#   segments(LL95,0.5,UL95,0.5, lwd=1)
#   title(main=paste("Equivalence bounds ",round(low_eqbound_d,digits=3)," and ",round(high_eqbound_d,digits=3),"\nEffect size = ",round(ES,digits=3)," \n TOST: ", 100*(1-alpha*2),"% CI [",round(LL90,digits=3),";",round(UL90,digits=3),"] ", TOSToutcome," \n NHST: ", 100*(1-alpha),"% CI [",round(LL95,digits=3),";",round(UL95,digits=3),"] ", testoutcome,sep=""), cex.main=1)
#   }
# 
# 
#   if(missing(verbose)) {
#     verbose <- TRUE
#   }
#   if(verbose == TRUE){
#     cat("TOST results:\n")
#     cat("Z-value lower bound:",format(Z1, digits = 3, nsmall = 2, scientific = FALSE),"\tp-value lower bound:",format(p1, digits = 1, nsmall = 3, scientific = FALSE))
#     cat("\n")
#     cat("Z-value upper bound:",format(Z2, digits = 3, nsmall = 2, scientific = FALSE),"\tp-value upper bound:",format(p2, digits = 1, nsmall = 3, scientific = FALSE))
#     cat("\n\n")
#     cat("Equivalence bounds (Cohen's d):")
#     cat("\n")
#     cat("low eqbound:", paste0(round(low_eqbound_d, digits = 4)),"\nhigh eqbound:",paste0(round(high_eqbound_d, digits = 4)))
#     cat("\n\n")
#     cat("TOST confidence interval:")
#     cat("\n")
#     cat("lower bound ",100*(1-alpha*2),"% CI: ", paste0(round(LL90, digits = 3)),"\nupper bound ",100*(1-alpha*2),"% CI:  ",paste0(round(UL90,digits = 3)), sep = "")
#     cat("\n\n")
#     cat("NHST confidence interval:")
#     cat("\n")
#     cat("lower bound ",100*(1-alpha),"% CI: ", paste0(round(LL95, digits = 3)),"\nupper bound ",100*(1-alpha),"% CI:  ",paste0(round(UL95,digits = 3)), sep = "")
#     cat("\n\n")
#     cat("Equivalence Test Result:\n")
#     message(cat("The equivalence test was ",TOSToutcome,", Z = ",format(Ztost, digits = 3, nsmall = 3, scientific = FALSE),", p = ",format(ptost, digits = 3, nsmall = 3, scientific = FALSE),", given equivalence bounds of ",format(low_eqbound_d, digits = 3, nsmall = 3, scientific = FALSE)," and ",format(high_eqbound_d, digits = 3, nsmall = 3, scientific = FALSE)," and an alpha of ",alpha,".",sep=""))
#     cat("\n")
#     cat("Null Hypothesis Test Result:\n")
#     message(cat("The null hypothesis test was ",testoutcome,", Z = ",format(Z, digits = 3, nsmall = 3, scientific = FALSE),", p = ",format(pttest, digits = 3, nsmall = 3, scientific = FALSE),", given an alpha of ",alpha,".",sep=""))
#     if(pttest <= alpha && ptost <= alpha){
#       combined_outcome <- "statistically different from zero and statistically equivalent to zero"
#     }
#     if(pttest < alpha && ptost > alpha){
#       combined_outcome <- "statistically different from zero and statistically not equivalent to zero"
#     }
#     if(pttest > alpha && ptost <= alpha){
#       combined_outcome <- "statistically not different from zero and statistically equivalent to zero"
#     }
#     if(pttest > alpha && ptost > alpha){
#       combined_outcome <- "statistically not different from zero and statistically not equivalent to zero"
#     }
#     cat("\n")
#     message(cat("Based on the equivalence test and the null-hypothesis test combined, we can conclude that the observed effect is ",combined_outcome,".",sep=""))
#   }
  
  # Return results in list()
  invisible(list(Q=Q, p_Q=p_Q, I2=I2 #,TOST_Z1=Z1,TOST_p1=p1,TOST_Z2=Z2,TOST_p2=p2,alpha=alpha,low_eqbound_d=low_eqbound_d,high_eqbound_d=high_eqbound_d, LL_CI_TOST=LL90,UL_CI_TOST=UL90,LL_CI_ZTEST=LL95,UL_CI_ZTEST=UL95
                 ))
  }

# Test whether function works:
TOST_out <- TOSTI(ES = Y, var = 1/W, low_eqbound_d = 0, high_eqbound_d = 10)

```

# Uncertainty around $I^2$

*There are several methods for obtaining an interval to convey uncertainty in $I^2$ . Because $I^2$ does not estimate any underlying quantity, these intervals would be better described as uncertainty intervals rather than confidence intervals. However, we will continue to describe them as confidence intervals since the distinction is not practically important. A simple method to obtain confidence intervals is as follows. First, if Q > (df þ 1), compute*

$$
B=0.5 \times \frac{\ln (Q)-\ln (d f)}{\sqrt{2 Q}-\sqrt{2 \times d f-1}}
$$

*or if Q  (df þ 1), compute* 

$$
B=\sqrt{\frac{1}{2 \times(d f-1) \times\left(1-\left(\frac{1}{3 \times(d f-1)^{2}}\right)\right)}}.
$$

*Then*

$$
L=\exp \left(0.5 \times \ln \left(\frac{Q}{d f}\right)-1.96 \times B\right)
$$
*and*

$$
U=\exp \left(0.5 \times \ln \left(\frac{Q}{d f}\right)+1.96 \times B\right).
$$
(Borenstein, p. 124).

*The 95% confidence intervals may then be obtained as*

$$
L L_{I^{2}}=\left(\frac{L^{2}-1}{L^{2}}\right) \times 100 \%
$$

*and*

$$
U L_{I^{2}}=\left(\frac{U^{2}-1}{U^{2}}\right) \times 100 \%.
$$
*Any value ($I^2$ , lower limit or upper limit) that is computed as less than zero is set to zero.* 
(Borenstein, p. 125).

**check CI for Q!**


So now include CI around $I^2$:

```{r}
# Compute B:
if (Q>df+1){
  B <- 0.5 * ((log(Q)-log(df))/(sqrt(2*Q)-sqrt(2*df-1))) #equation 16.20, Borenstein, p. 124 
} else {
  B <- sqrt(1/(2*(df-1)*(1-(1/(3*(df-1)^2)))))
}

# Compute intermediates L and U:
alpha <- 0.05
z_alpha <- qnorm(1-alpha/2)
L <- exp(0.5*log(Q/df)-z_alpha*B)
U <- exp(0.5*log(Q/df)+z_alpha*B)

# Compute lower and upper limit of the CI:
LL <- 100* (L^2-1)/L^2
UL <- 100* (U^2-1)/U^2

# Set limit to zero for negative values:
if (LL<0){LL <- 0}
if (UL<0){UL <- 0}

```


# Equivalence

For F tests, we ask the question "*can we reject the hypothesis that the total proportion of variance in Y attributable to X is greater than or equal to $\Delta$?*" (https://arxiv.org/pdf/1905.11875v1.pdf, p. 5). And we want to translate this to the $I^2$ statistic:

$$
H_{0} : 100 \% > I^2 \geq \Delta \\

H_{1} : 0\% \leq I^2 < \Delta
$$
where $\Delta$ represents the SESOI in percentages. 

However, we do not have a known distribution for $I^2$, so we use Q instead. Similar to the F-statistic non-inferiority test, a p-value for this distribution is obtained by inverting the one-sided CI around Q. But then we also need to specify $\Delta$ in units of Q.

If $I^2 = 100*(Q-df)/Q$, then $I^2/100 = 1 - (df/Q)$, and $Q = -df/((I^2/100)-1) $, where $I^2/100 \neq 1$ and $df \neq 0$. In meta-analyses, there are at least 2 studies (k>1), so df (k-1) is never equal to zero. Alternatively written, $Q = -(100*df)/(I^2-100)$, where $I^2 \neq 100$.


```{r}
# Convert Delta to units of Q:
Delta <- 10
D_Q <- -(100*df)/(Delta-100)
# pval = pchisq(Q,df,ncp=),lower.tail=TRUE)

# Visually inspect the effect of the ncp:
curve(dchisq(x, df=29), col='red', main = "Chi-Square Density Graph", from=0,to=60)
xvec <- seq(7.5,60,length=101)
pvec <- dchisq(xvec,df=29, ncp = D_Q-df)
polygon(c(xvec,rev(xvec)),c(pvec,rep(0,length(pvec))), col=adjustcolor("black",alpha=0.3))

# Look at CI of chi sq distr:
mean(qchisq(c(.025,.975),df=df, ncp = D_Q, lower.tail=FALSE))


```

How to get the ncp for Q?

*chi-square distribution with $k-1$ degrees of freedom and non-centrality parameter equal to*

$$
\lambda=\sum_{i=1}^{k} w_{i}\left(\theta_{i}-\overline{\theta}_{\bullet}\right)^{2}
$$

*where $\overline{\theta}_{\bullet}$ is the weighted mean of $\theta_1, \theta_2, ..., \theta_k$, and can be written as*

$$
\overline{\theta}_{\bullet}=\frac{\sum_{i=1}^{k} w_{i} \theta_{i}}{\sum_{i=1}^{k} w_{i}}.
$$

(Pigott, p. 57).

Apply to current data:

```{r}
theta_dot <- sum(W*Y)/sum(W)
lambda <- sum(W*(Y-theta_dot)^2)


```

